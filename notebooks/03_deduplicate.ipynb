{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a365dd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### NOTEBOOK 3: 03_deduplicate.ipynb ###\n",
    "#\n",
    "# GOAL: Memuat file master yang sudah bersih dan menerapkan\n",
    "#       deduplikasi 3-tahap (ID, Price/Address, Geospatial)\n",
    "#       untuk membuat dataset final.\n",
    "#\n",
    "# INPUT: master_cleaned_features.csv\n",
    "# OUTPUT: bandung_housing_FINAL.csv\n",
    "#\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "\n",
    "# Geospatial\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.options.mode.chained_assignment = None\n",
    "tqdm.pandas()\n",
    "\n",
    "print(\"--- 03_deduplicate.ipynb ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe23b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---\n",
    "# ## Step 1: File Paths\n",
    "# ---\n",
    "\n",
    "print(\"Step 1: Setting up file paths...\")\n",
    "\n",
    "# Path Definitions\n",
    "PROJECT_ROOT = Path(r\"..\")\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "PROCESSED_DIR = DATA_DIR / \"processed\"\n",
    "NOTEBOOK_DIR = PROJECT_ROOT / \"notebooks\" # Lokasi notebook ini\n",
    "\n",
    "# --- INPUT FILE (Failsafe dari Notebook 2) ---\n",
    "CLEANED_MASTER_PATH = PROCESSED_DIR / \"master_cleaned_features.csv\"\n",
    "\n",
    "# --- OUTPUT FILE (Final) ---\n",
    "FINAL_OUTPUT_PATH = PROCESSED_DIR / \"bandung_housing_FINAL.csv\"\n",
    "\n",
    "print(f\"Input: {CLEANED_MASTER_PATH}\")\n",
    "print(f\"Output: {FINAL_OUTPUT_PATH}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a95bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---\n",
    "# ## Step 2: Load Cleaned Master File\n",
    "# ---\n",
    "\n",
    "print(\"Step 2: Loading Cleaned Master File...\")\n",
    "\n",
    "try:\n",
    "    # Tentukan tipe data untuk kolom yang bermasalah saat load\n",
    "    dtypes = {'zipcode': 'str', 'geo_confidence': 'str'}\n",
    "    df_master = pd.read_csv(CLEANED_MASTER_PATH, dtype=dtypes)\n",
    "    print(f\"Loaded {len(df_master)} records from {CLEANED_MASTER_PATH}\\n\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"\u274c ERROR: File not found at {CLEANED_MASTER_PATH}. Aborting.\")\n",
    "    print(\"Pastikan Notebook 02 telah berhasil dijalankan.\")\n",
    "    sys.exit()\n",
    "except Exception as e:\n",
    "    print(f\"\u274c ERROR loading {CLEANED_MASTER_PATH}: {e}\")\n",
    "    sys.exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a1176d",
   "metadata": {},
   "source": [
    "Data Cleaning\n",
    "\n",
    "Next code is a 3\u2011stage \"filter\" to remove duplicates, with each stage becoming stricter.\n",
    "\n",
    "Stage 1: Duplicates by ID (id)  \n",
    "The most basic cleaning. The code looks for rows with the exact same id and removes all duplicates, keeping only the first (keep='first').  \n",
    "\n",
    "Why: This captures the most obvious duplicates, where the same listing may have been scraped more than once, or recreated during the merging process.  \n",
    "\n",
    "---\n",
    "\n",
    "Stage 2: Duplicates by Price & Address (price, master_address)  \n",
    "It looks for rows that have the EXACT SAME price AND the EXACT SAME master_address.  \n",
    "\n",
    "Why: This captures \"lazy duplicates.\" Imagine an agent posting the same ad on different property sites. Stage 1 would not catch this (because the ids differ), but Stage 2 does. It is a crucial step to ensure that every unique property is treated as a single data point. \n",
    "\n",
    "---\n",
    "\n",
    "Stage 3: Geospatial Deduplication (Aggressive Logic)  \n",
    "This is the most complex and important part, and will remove a great deal of data, but is a necessary process to make the data usable for analysis. \n",
    "\n",
    "1. Data Preparation  \n",
    "The code splits the data into two groups:  \n",
    "- clusterable_gdf: \"good\" listings (valid latitude, longitude, price, and land_size_sqm).  \n",
    "- non_clusterable_df: \"bad\" listings (e.g., missing coordinates).  \n",
    "\n",
    "The \"bad\" listings are safely stored and added back at the end. No data is lost (yet).  \n",
    "\n",
    "2. Clustering (DBSCAN)  \n",
    "The code uses an algorithm called DBSCAN. Around each data point, it puts a circle with a radius of 100 meters (eps=100).  \n",
    "\n",
    "- If at least 2 listings (min_samples=2) fall within each other\u2019s circle, they are grouped together and assigned a cluster ID (e.g., cluster 5).  \n",
    "- If a listing stands alone (no other listing within 100m), it is labeled \"noise\" (cluster = -1).  \n",
    "\n",
    "3. Multi-Factor Duplicate Check   \n",
    "This is where the \"data decimation\" happens. The code iterates through each cluster:  \n",
    "- A listing is removed only if it matches another listing in the same cluster on all key features.  \n",
    "- A \"true duplicate\" means:  \n",
    "  - Same `price`  \n",
    "  - AND same `land_size_sqm`  \n",
    "  - AND same `building_size_sqm`\n",
    "\n",
    "I did this to remove only *true duplicates* while keeping valid, unique properties.  \n",
    "Many listings can appear close together on the map, but that doesn\u2019t always mean they are the same house or apartment. If we treated all clustered ads as duplicates, we would risk deleting real properties in the same building or complex.  \n",
    "\n",
    "By checking for the same `price`, `land_size_sqm`, and `building_size_sqm`, the filter only removes ads that are truly identical. This avoids bias in the dataset, where one property could be counted too many times or valid properties could be lost.  \n",
    "\n",
    "This logic is not always \u201cgood.\u201d It is a trade-off:  \n",
    "- **Good** when clusters contain repeated ads for the same property, since it reduces spam and improves accuracy.  \n",
    "- **Risky** if the chosen features miss small differences between properties, which could let some duplicates stay or wrongly remove rare cases.  \n",
    "\n",
    "4. Merging Back  \n",
    "The final dataset is a combination of:  \n",
    "- All \"noise\" listings (unique, stand\u2011alone properties).  \n",
    "- All \"non\u2011clusterable\" listings (the \"bad\" data without coordinates) that were saved earlier.  \n",
    "- From within the clusters, all unique properties are kept. Only the true identical\u2011feature duplicates are removed.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c655cee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---\n",
    "# ## Step 3: 3-Stage Deduplication\n",
    "# ---\n",
    "\n",
    "print(\"---\")\n",
    "print(\"## Step 3: Running 3-Stage Deduplication\")\n",
    "print(\"---\")\n",
    "\n",
    "def geospatial_deduplication(df, cluster_radius_m=100):\n",
    "    \"\"\"\n",
    "    Menjalankan deduplikasi geospasial menggunakan DBSCAN.\n",
    "    \n",
    "    LOGIKA BARU (SMART):\n",
    "    Hanya menghapus listing di dalam sebuah cluster jika listing tersebut\n",
    "    memiliki 'price', 'land_size_sqm', DAN 'building_size_sqm'\n",
    "    yang SAMA PERSIS dengan listing lain di cluster yang sama.\n",
    "    \n",
    "    Ini memecahkan \"Masalah Gedung Apartemen\" dan \"Masalah Heuristik Buruk\".\n",
    "    \"\"\"\n",
    "    print(f\"Starting SMART geospatial deduplication with {len(df)} listings...\")\n",
    "    gdf = df.copy()\n",
    "    \n",
    "    # 1. Siapkan data untuk clustering\n",
    "    gdf['latitude'] = pd.to_numeric(gdf['latitude'], errors='coerce')\n",
    "    gdf['longitude'] = pd.to_numeric(gdf['longitude'], errors='coerce')\n",
    "    gdf['price'] = pd.to_numeric(gdf['price'], errors='coerce')\n",
    "    gdf['land_size_sqm'] = pd.to_numeric(gdf['land_size_sqm'], errors='coerce')\n",
    "    gdf['building_size_sqm'] = pd.to_numeric(gdf['building_size_sqm'], errors='coerce') # Perlu untuk logika baru\n",
    "\n",
    "    # Hanya cluster baris yang memiliki koordinat\n",
    "    gdf['can_cluster'] = gdf['latitude'].notna() & gdf['longitude'].notna()\n",
    "    \n",
    "    clusterable_gdf = gdf[gdf['can_cluster']].copy()\n",
    "    non_clusterable_df = gdf[~gdf['can_cluster']].copy() # Simpan baris yang tidak bisa di-cluster\n",
    "    \n",
    "    if clusterable_gdf.empty:\n",
    "        print(\"No data with valid coordinates to cluster.\")\n",
    "        return df\n",
    "        \n",
    "    # 2. Proyeksikan ke CRS meteran (Mercator) untuk DBSCAN\n",
    "    clusterable_gdf['geometry'] = [Point(xy) for xy in zip(clusterable_gdf['longitude'], clusterable_gdf['latitude'])]\n",
    "    clusterable_gdf = gpd.GeoDataFrame(clusterable_gdf, geometry='geometry', crs=\"EPSG:4326\")\n",
    "    clusterable_gdf = clusterable_gdf.to_crs(\"EPSG:3857\") # Proyeksi Mercator (dalam meter)\n",
    "    \n",
    "    # 3. Jalankan DBSCAN\n",
    "    coords = np.array(list(zip(clusterable_gdf.geometry.x, clusterable_gdf.geometry.y)))\n",
    "    db = DBSCAN(eps=cluster_radius_m, min_samples=2, metric='euclidean').fit(coords)\n",
    "    clusterable_gdf['cluster'] = db.labels_\n",
    "    \n",
    "    print(f\"Found {len(set(db.labels_)) - (1 if -1 in db.labels_ else 0)} geospatial clusters.\")\n",
    "\n",
    "    # 4. Terapkan Logika Deduplikasi BARU (SMART)\n",
    "    \n",
    "    # --- AWAL DARI LOGIKA BARU ---\n",
    "    # Logika ini didasarkan pada file .docx Anda:\n",
    "    # \"nearby AND same price AND same land_size AND same building_size\"\n",
    "    \n",
    "    keep_indices = set() # Set untuk menyimpan indeks baris yang ingin *disimpan*\n",
    "\n",
    "    # Simpan semua baris yang \"noise\" (tidak termasuk dalam cluster manapun)\n",
    "    noise_indices = set(clusterable_gdf[clusterable_gdf['cluster'] == -1].index)\n",
    "    keep_indices.update(noise_indices)\n",
    "\n",
    "    # Iterasi HANYA melalui cluster yang valid (bukan -1)\n",
    "    for cluster_id in set(clusterable_gdf['cluster']):\n",
    "        if cluster_id == -1:\n",
    "            continue\n",
    "            \n",
    "        cluster_listings = clusterable_gdf[clusterable_gdf['cluster'] == cluster_id]\n",
    "        \n",
    "        # Tentukan kolom untuk memeriksa duplikasi\n",
    "        # Kita hanya peduli tentang duplikat jika fitur-fitur ini SAMA PERSIS.\n",
    "        duplicate_check_cols = ['price', 'land_size_sqm', 'building_size_sqm']\n",
    "        \n",
    "        # Gunakan drop_duplicates pada subset cluster\n",
    "        # Ini akan menyimpan *satu* salinan dari setiap kombinasi unik\n",
    "        # 'price'/'land_size'/'building_size' di dalam cluster tersebut.\n",
    "        survivor_listings = cluster_listings.drop_duplicates(\n",
    "            subset=duplicate_check_cols,\n",
    "            keep='first'\n",
    "        )\n",
    "        \n",
    "        # Tambahkan indeks dari listing yang selamat ke set kita\n",
    "        keep_indices.update(survivor_listings.index)\n",
    "    \n",
    "    # --- AKHIR DARI LOGIKA BARU ---\n",
    "\n",
    "    # 5. Gabungkan kembali DataFrame\n",
    "    final_clustered_df = df.loc[list(keep_indices)] # Ambil baris yang disimpan dari df asli\n",
    "    final_df = pd.concat([final_clustered_df, non_clusterable_df], ignore_index=True) # Tambahkan kembali baris yang tidak di-cluster\n",
    "    \n",
    "    print(f\"SMART geospatial deduplication removed {len(df) - len(final_df)} listings.\")\n",
    "    return final_df\n",
    "\n",
    "# ========================================================================\n",
    "# ---                 AKHIR DARI FUNGSI YANG DIPERBAIKI                ---\n",
    "# ========================================================================\n",
    "\n",
    "\n",
    "# --- Menjalankan 3-Tahap Deduplikasi ---\n",
    "\n",
    "# Tahap A: Deduplikasi berdasarkan 'id' unik\n",
    "print(\"Running Stage A: Deduplicate by 'id'...\")\n",
    "count_a = len(df_master)\n",
    "df_master.drop_duplicates(subset=['id'], keep='first', inplace=True)\n",
    "print(f\"Removed {count_a - len(df_master)} duplicates by 'id'.\\n\")\n",
    "\n",
    "# Tahap B: Deduplikasi berdasarkan 'price' dan 'master_address'\n",
    "print(\"Running Stage B: Deduplicate by 'price' & 'master_address'...\")\n",
    "count_b = len(df_master)\n",
    "df_master.drop_duplicates(subset=['price', 'master_address'], keep='first', inplace=True)\n",
    "print(f\"Removed {count_b - len(df_master)} duplicates by 'price'/'master_address'.\\n\")\n",
    "\n",
    "# Tahap C: Deduplikasi Geospasial (Logika SMART)\n",
    "print(\"Running Stage C: Geospatial Deduplication (SMART Logic)...\")\n",
    "df_master = geospatial_deduplication(df_master, cluster_radius_m=100) # radius 100m\n",
    "print(\"SMART Geospatial deduplication complete.\\n\")\n",
    "\n",
    "print(f\"Total listings remaining after 3-stage deduplication: {len(df_master)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5557d2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---\n",
    "# ## Step 4: Save Final File\n",
    "# ---\n",
    "\n",
    "print(\"---\")\n",
    "print(\"## Step 4: Saving Final File\")\n",
    "print(\"---\")\n",
    "\n",
    "try:\n",
    "    # Tentukan urutan kolom akhir\n",
    "    final_columns = [\n",
    "        'id', 'source', 'price', 'master_address', 'latitude', 'longitude', 'zipcode',\n",
    "        'bedrooms', 'bathrooms', 'land_size_sqm', 'building_size_sqm',\n",
    "        'description', 'specs', 'url', 'scraped_at', 'geo_confidence'\n",
    "    ]\n",
    "    \n",
    "    # Filter kolom yang ada di dataframe\n",
    "    final_columns_exist = [col for col in final_columns if col in df_master.columns]\n",
    "    df_final = df_master[final_columns_exist]\n",
    "\n",
    "    # Simpan ke nama file output yang *asli*\n",
    "    df_final.to_csv(FINAL_OUTPUT_PATH, index=False, encoding='utf-8-sig')\n",
    "    \n",
    "    print(f\"\\n\u2705\u2705\u2705 03_deduplicate.ipynb COMPLETE! \u2705\u2705\u2705\")\n",
    "    print(f\"Final dataset saved to:\")\n",
    "    print(f\"{FINAL_OUTPUT_PATH}\")\n",
    "    print(f\"\\nTotal listings in final file: {len(df_final)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\u274c ERROR: Failed to save final file. {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bandung-housing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}