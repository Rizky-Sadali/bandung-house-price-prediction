{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1246e999",
   "metadata": {},
   "outputs": [],
   "source": [
    "### NOTEBOOK 2: 02_merge_and_clean.ipynb ###\n",
    "#\n",
    "# GOAL: Memuat file \"failsafe\" yang sudah di-geocode, membersihkan\n",
    "#       bug 'description' Platform B, menggabungkannya, dan menjalankan\n",
    "#       feature engineering (Zipcode Fix & Waterfall).\n",
    "#\n",
    "# INPUT: platform_a_geocoded.csv, platform_b_geocoded.csv\n",
    "# OUTPUT: master_cleaned_features.csv\n",
    "#\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from thefuzz import fuzz # Dibutuhkan untuk Zipcode Fix\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.options.mode.chained_assignment = None\n",
    "tqdm.pandas()\n",
    "\n",
    "print(\"--- 02_merge_and_clean.ipynb ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b0192d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Step 1: File Paths\n",
    "# ---\n",
    "\n",
    "print(\"Step 1: Setting up file paths...\")\n",
    "\n",
    "# Path Definitions\n",
    "PROJECT_ROOT = Path(r\"..\")\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "PROCESSED_DIR = DATA_DIR / \"processed\"\n",
    "NOTEBOOK_DIR = PROJECT_ROOT / \"notebooks\" # Lokasi notebook ini\n",
    "\n",
    "# --- INPUT FILES (Failsafes dari Notebook 1) ---\n",
    "R123_GEOCODED_PATH = PROCESSED_DIR / \"platform_a_geocoded.csv\"\n",
    "PLATFORM_B_GEOCODED_PATH = PROCESSED_DIR / \"platform_b_geocoded.csv\"\n",
    "\n",
    "# --- OUTPUT FILE (Failsafe baru) ---\n",
    "CLEANED_MASTER_PATH = PROCESSED_DIR / \"master_cleaned_features.csv\"\n",
    "\n",
    "print(f\"Input 1: {R123_GEOCODED_PATH}\")\n",
    "print(f\"Input 2: {PLATFORM_B_GEOCODED_PATH}\")\n",
    "print(f\"Output: {CLEANED_MASTER_PATH}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad563ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---\n",
    "# ## Step 2: Load, Fix, and Merge\n",
    "# ---\n",
    "\n",
    "print(\"Step 2: Loading, Fixing, and Merging Data...\")\n",
    "\n",
    "try:\n",
    "    # Tentukan tipe data untuk kolom yang bermasalah saat load\n",
    "    r123_dtypes = {'zipcode': 'str', 'geo_confidence': 'str'}\n",
    "    df_r123 = pd.read_csv(R123_GEOCODED_PATH, dtype=r123_dtypes)\n",
    "    print(f\"Loaded {len(df_r123)} records from platform_a_geocoded.csv\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"\u274c ERROR: File not found at {R123_GEOCODED_PATH}. Aborting.\")\n",
    "    sys.exit()\n",
    "except Exception as e:\n",
    "    print(f\"\u274c ERROR loading {R123_GEOCODED_PATH}: {e}\")\n",
    "    sys.exit()\n",
    "\n",
    "try:\n",
    "    # Tentukan tipe data untuk kolom yang bermasalah saat load\n",
    "    ninetynineco_dtypes = {'zipcode': 'str', 'geo_confidence': 'str', 'description': 'str'}\n",
    "    df_platform_b = pd.read_csv(PLATFORM_B_GEOCODED_PATH, dtype=ninetynineco_dtypes)\n",
    "    print(f\"Loaded {len(df_platform_b)} records from platform_b_geocoded.csv\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"\u274c ERROR: File not found at {PLATFORM_B_GEOCODED_PATH}. Aborting.\")\n",
    "    sys.exit()\n",
    "except Exception as e:\n",
    "    print(f\"\u274c ERROR loading {PLATFORM_B_GEOCODED_PATH}: {e}\")\n",
    "    sys.exit()\n",
    "\n",
    "\n",
    "# --- *** CRITICAL FIX: Membersihkan Kolom Deskripsi Platform B *** ---\n",
    "if 'description' in df_platform_b.columns and 'description_clean' in df_platform_b.columns:\n",
    "    print(\"Fixing Platform B description columns (dropping 'description', renaming 'description_clean')...\")\n",
    "    df_platform_b = df_platform_b.drop(columns=['description'])\n",
    "    df_platform_b = df_platform_b.rename(columns={'description_clean': 'description'})\n",
    "else:\n",
    "    print(\"Warning: Kolom 'description' dan 'description_clean' tidak ditemukan, mungkin sudah diperbaiki.\")\n",
    "# ----------------------------------------------------------------\n",
    "\n",
    "# --- Grand Merge ---\n",
    "print(\"Merging Platform B and Platform A dataframes...\")\n",
    "df_master = pd.concat([df_platform_b, df_r123], ignore_index=True)\n",
    "\n",
    "# Buat satu kolom alamat master untuk pencocokan\n",
    "df_master['master_address'] = df_master['geo_address'].fillna(df_master['address']).astype(str)\n",
    "\n",
    "print(f\"Successfully merged. Total listings: {len(df_master)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e073114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---\n",
    "# ## Step 3: Unified Zipcode Fix (Logika Anda dari IPYNB #8)\n",
    "# ---\n",
    "\n",
    "print(\"---\")\n",
    "print(\"## Step 3: Unified Zipcode Fix\")\n",
    "print(\"---\")\n",
    "\n",
    "def find_best_match_hierarchical(target_row, source_df):\n",
    "    \"\"\"\n",
    "    Menemukan pencocokan zipcode terbaik menggunakan lokasi fuzzy dan harga sebagai tie-breaker.\n",
    "    \"\"\"\n",
    "    target_location = target_row['master_address']\n",
    "    target_price = target_row['price']\n",
    "\n",
    "    # 1. Temukan pencocokan lokasi terbaik\n",
    "    location_scores = source_df['master_address'].apply(\n",
    "        lambda source_loc: fuzz.token_sort_ratio(target_location, str(source_loc))\n",
    "    )\n",
    "    max_loc_score = location_scores.max()\n",
    "    \n",
    "    # Filter hanya ke pencocokan lokasi terbaik\n",
    "    best_location_matches = source_df[location_scores == max_loc_score]\n",
    "\n",
    "    # 2. Gunakan Harga sebagai tie-breaker\n",
    "    if len(best_location_matches) == 1:\n",
    "        best_match = best_location_matches.iloc[0]\n",
    "    else:\n",
    "        # Pastikan harga adalah numerik untuk perbandingan\n",
    "        best_location_matches['price'] = pd.to_numeric(best_location_matches['price'], errors='coerce')\n",
    "        price_differences = (best_location_matches['price'] - target_price).abs()\n",
    "        best_match = best_location_matches.loc[price_differences.idxmin()]\n",
    "        \n",
    "    return pd.Series({\n",
    "        'zipcode_fuzzy': best_match['zipcode'],\n",
    "        'zipcode_match_score': max_loc_score\n",
    "    })\n",
    "\n",
    "def fix_missing_zipcodes(df):\n",
    "    \"\"\"Menerapkan logika pencocokan fuzzy untuk mengisi NaN di 'zipcode'.\"\"\"\n",
    "    print(\"Running Unified Zipcode Fix...\")\n",
    "    df['price'] = pd.to_numeric(df['price'], errors='coerce')\n",
    "    \n",
    "    # Konversi 'zipcode' ke string untuk konsistensi, ubah NaNs menjadi placeholder\n",
    "    df['zipcode'] = df['zipcode'].astype(str).replace('nan', np.nan)\n",
    "\n",
    "    source_df = df.dropna(subset=['zipcode', 'master_address', 'price']).copy()\n",
    "    target_df = df[df['zipcode'].isna() & df['master_address'].notna() & df['price'].notna()].copy()\n",
    "    \n",
    "    if target_df.empty:\n",
    "        print(\"No missing zipcodes to fix. Skipping.\")\n",
    "        return df\n",
    "        \n",
    "    print(f\"Applying fuzzy match to fix {len(target_df)} missing zipcodes...\")\n",
    "    \n",
    "    match_results = target_df.progress_apply(\n",
    "        lambda row: find_best_match_hierarchical(row, source_df),\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    df = df.join(match_results)\n",
    "    \n",
    "    # Isi kolom 'zipcode' asli dengan yang dari fuzzy-matched\n",
    "    df['zipcode'] = df['zipcode'].fillna(df['zipcode_fuzzy'])\n",
    "    \n",
    "    print(\"Fuzzy matching for zipcodes complete.\\n\")\n",
    "    return df\n",
    "\n",
    "# Jalankan fungsinya\n",
    "df_master = fix_missing_zipcodes(df_master)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c40dd9",
   "metadata": {},
   "source": [
    "Filling Missing Values\n",
    "\n",
    "\"Waterfall\" Logic  \n",
    "For each row in the master data, I run these 3 checks:\n",
    "\n",
    "1. Check Main Column (e.g., bathrooms)  \n",
    "First, I check if the bathrooms column already has a value.  \n",
    "If Yes: (For example, the scraper Platform A already provided 3.0). I keep this value. The process for this row is finished.  \n",
    "If No: (The value is NaN or empty), the data \"falls\" to the next step.  \n",
    "\n",
    "2. Parse specs Column (JSON)  \n",
    "Next, I call the parse_specs function. This function looks inside the specs column (which contains a JSON string such as {\"bathrooms\": \"2\", \"land area\": \"100 m\u00b2\", ...}).  \n",
    "If Yes: I search for the aliases I defined (such as bathrooms, bath, or km). If found, I extract the value (e.g., \"2\"), clean it into the number 2, and use it to fill the empty bathrooms column. Process finished.  \n",
    "If No: (The JSON does not contain a bathroom alias or the specs column itself is empty), the data \"falls\" to the last step.  \n",
    "\n",
    "3. Parse description Column (Text)  \n",
    "Finally, I call the parse_description function. This function uses Regular Expressions (Regex) to \"read\" the description text (for example, \"Nice house in Setiabudi... 2 bath... 3 bed...\").  \n",
    "If Yes: I search for the patterns I defined (such as (\\d+)\\s*bath or (\\d+)\\s*bathrooms). If found, I extract the number (e.g., 2 from \"2 bath\"), and use it to fill the bathrooms column.  \n",
    "If No: (No pattern found in the text), I give up.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ce8110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---\n",
    "# ## Step 4: \"Waterfall\" Feature Engineering (Logika Anda dari IPYNB #9)\n",
    "# ---\n",
    "\n",
    "print(\"---\")\n",
    "print(\"## Step 4: 'Waterfall' Feature Engineering\")\n",
    "print(\"---\")\n",
    "\n",
    "def clean_value(value_str):\n",
    "    try:\n",
    "        # Hapus 'm\u00b2' atau 'm' lalu ambil angkanya\n",
    "        value_str = str(value_str).lower().replace('m\u00b2', '').replace('m', '').strip()\n",
    "        match = re.search(r'([\\d\\.]+)', value_str) # Ambil angka, bisa jadi float\n",
    "        if match: \n",
    "            return int(float(match.group(0))) # Ubah ke float dulu, lalu int\n",
    "    except (TypeError, ValueError, AttributeError): pass\n",
    "    return np.nan\n",
    "\n",
    "def parse_specs(specs_str, aliases):\n",
    "    try:\n",
    "        specs_dict = json.loads(str(specs_str).lower().replace(\"'\", '\"'))\n",
    "        specs_keys_lower = {k.lower(): v for k, v in specs_dict.items()}\n",
    "        for alias in aliases:\n",
    "            if alias in specs_keys_lower:\n",
    "                return clean_value(specs_keys_lower[alias])\n",
    "    except (json.JSONDecodeError, TypeError, AttributeError): pass\n",
    "    return np.nan\n",
    "\n",
    "def parse_description(description_str, patterns):\n",
    "    try:\n",
    "        text = str(description_str).lower()\n",
    "        for pattern in patterns:\n",
    "            match = re.search(pattern, text)\n",
    "            if match:\n",
    "                for group in match.groups():\n",
    "                    if group: return clean_value(group)\n",
    "    except (TypeError, AttributeError): pass\n",
    "    return np.nan\n",
    "\n",
    "def fill_data_waterfall(df, column, specs_aliases, desc_patterns):\n",
    "    # Pastikan kolom target ada\n",
    "    if column not in df.columns: \n",
    "        df[column] = np.nan\n",
    "        \n",
    "    df[column] = pd.to_numeric(df[column], errors='coerce')\n",
    "    initial_missing = df[column].isna().sum()\n",
    "    if initial_missing == 0:\n",
    "        print(f\"-> No missing values for '{column}'. Skipping.\")\n",
    "        return df\n",
    "\n",
    "    def waterfall_filler(row):\n",
    "        # 1. Cek nilai yang sudah ada\n",
    "        if pd.notna(row[column]): return row[column]\n",
    "        # 2. Parse 'specs'\n",
    "        value = parse_specs(row['specs'], specs_aliases)\n",
    "        if pd.notna(value): return value\n",
    "        # 3. Parse 'description' (sekarang sudah bersih)\n",
    "        value = parse_description(row['description'], desc_patterns)\n",
    "        if pd.notna(value): return value\n",
    "        return np.nan\n",
    "\n",
    "    print(f\"Filling {initial_missing} missing values for '{column}'...\")\n",
    "    df[column] = df.progress_apply(waterfall_filler, axis=1)\n",
    "    filled_count = initial_missing - df[column].isna().sum()\n",
    "    print(f\"-> Filled {filled_count} missing values for '{column}'.\")\n",
    "    return df\n",
    "\n",
    "# Tentukan fitur dan aliasnya\n",
    "feature_cols = ['bedrooms', 'bathrooms', 'land_size_sqm', 'building_size_sqm']\n",
    "all_aliases = {\n",
    "    'bedrooms': (['kamar tidur', 'kt', 'bedrooms'], [r'(\\d+)\\s*kt', r'(\\d+)\\s*kamar tidur']),\n",
    "    'bathrooms': (['kamar mandi', 'km', 'bathrooms'], [r'(\\d+)\\s*km', r'(\\d+)\\s*kamar mandi']),\n",
    "    'land_size_sqm': (['luas tanah', 'lt', 'land size', 'luas lahan'], [r'lt\\s*(\\d+)', r'luas tanah\\s*(\\d+)']),\n",
    "    'building_size_sqm': (['luas bangunan', 'lb', 'building size'], [r'lb\\s*(\\d+)', r'luas bangunan\\s*(\\d+)'])\n",
    "}\n",
    "\n",
    "# Jalankan waterfall untuk setiap fitur\n",
    "for col in feature_cols:\n",
    "    aliases, patterns = all_aliases[col]\n",
    "    df_master = fill_data_waterfall(df_master, col, aliases, patterns)\n",
    "\n",
    "print(\"Waterfall feature engineering complete.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f13fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"---\")\n",
    "print(\"## Step 5: Saving Failsafe File\")\n",
    "print(\"---\")\n",
    "\n",
    "try:\n",
    "    # Tipe data akhir sebelum menyimpan\n",
    "    final_numeric_cols = ['price', 'bedrooms', 'bathrooms', 'land_size_sqm', 'building_size_sqm', 'latitude', 'longitude']\n",
    "    for col in final_numeric_cols:\n",
    "        if col in df_master.columns:\n",
    "            df_master[col] = pd.to_numeric(df_master[col], errors='coerce')\n",
    "            \n",
    "    # Konversi kolom Int yang bisa nullable\n",
    "    for col in ['bedrooms', 'bathrooms', 'land_size_sqm', 'building_size_sqm']:\n",
    "         if col in df_master.columns:\n",
    "            df_master[col] = df_master[col].astype('Int64')\n",
    "\n",
    "    df_master.to_csv(CLEANED_MASTER_PATH, index=False, encoding='utf-8-sig')\n",
    "    \n",
    "    print(f\"\\n\u2705\u2705\u2705 02_merge_and_clean.ipynb COMPLETE! \u2705\u2705\u2705\")\n",
    "    print(f\"New failsafe file saved to:\")\n",
    "    print(f\"{CLEANED_MASTER_PATH}\")\n",
    "    print(f\"\\nTotal listings in file: {len(df_master)}\")\n",
    "    print(\"Ready for Notebook 3 (Deduplication).\")\n",
    "except Exception as e:\n",
    "    print(f\"\u274c ERROR: Failed to save file. {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bandung-housing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}